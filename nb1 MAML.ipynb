{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"nb1.ipynb","provenance":[],"collapsed_sections":["uu0iK7SU_lWd","jCzh8pDl_lWj","-5485YZk_lWl","5eLfCnTA_lWr","A2RmBKMp_lWt"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"uu0iK7SU_lWd"},"source":["# MAML - MODEL-AGNOSTIC META-LEARNING"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ay2Tf75g_yTy","executionInfo":{"status":"ok","timestamp":1617944713040,"user_tz":-330,"elapsed":1040,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"7d20b668-d1c8-4549-f1ee-d476aa6aa2a4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrmJT523_lWg","executionInfo":{"status":"ok","timestamp":1617944713637,"user_tz":-330,"elapsed":1613,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"5289253b-6531-4084-ad9d-e2a008a0485d"},"source":["%cd drive/MyDrive/'Colab Notebooks/MetaLearning'\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/MetaLearning\n","l2lutils.ipynb\tmodels.ipynb  nb1.ipynb  nb2-CNP.ipynb\tnb3.ipynb  utils.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g8Ne-r4t_lWh","executionInfo":{"status":"ok","timestamp":1617944720127,"user_tz":-330,"elapsed":8096,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["!pip install import_ipynb --quiet\n","!pip install learn2learn --quiet"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"id":"sZqiJ5vw_lWi","executionInfo":{"status":"ok","timestamp":1617944721136,"user_tz":-330,"elapsed":9097,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"0d4a30c5-a401-4698-c0ea-3fe5f8332760"},"source":["import import_ipynb\n","import utils\n","import models\n","# import l2lutils\n","utils.hide_toggle('Imports 1')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["importing Jupyter notebook from utils.ipynb\n","importing Jupyter notebook from models.ipynb\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["\n","        <script>\n","            function code_toggle_16975314248369966566() {\n","                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n","            }\n","\n","            \n","        </script>\n","\n","        <a href=\"javascript:code_toggle_16975314248369966566()\"><b>Imports 1</b> (show/hide)</a>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"NfZ2eJjc_lWj","executionInfo":{"status":"ok","timestamp":1617944721137,"user_tz":-330,"elapsed":9090,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"cc68b031-d336-4fbf-cd9e-444925bb9431"},"source":["from IPython import display\n","import torch\n","import torch.nn as nn\n","from sklearn.manifold import TSNE\n","from matplotlib import pyplot as plt\n","# from l2lutils import KShotLoader\n","from IPython import display\n","utils.hide_toggle('Imports 2')"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <script>\n","            function code_toggle_9019987241048824393() {\n","                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n","            }\n","\n","            \n","        </script>\n","\n","        <a href=\"javascript:code_toggle_9019987241048824393()\"><b>Imports 2</b> (show/hide)</a>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"CenNHTYQnmyy"},"source":["l2lutils"]},{"cell_type":"code","metadata":{"id":"AMIoG86Klnzb","executionInfo":{"status":"ok","timestamp":1617944721708,"user_tz":-330,"elapsed":9652,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["import torch\n","import numpy as np\n","import learn2learn as l2l\n","from learn2learn.data import *\n","import import_ipynb\n","import utils\n","class KShotLoader():\n","    def __init__(self,myds,num_tasks=1000,shots=2,ways=2,classes=None):\n","        self.shots = shots\n","        self.ways = ways\n","        self.myMds = l2l.data.MetaDataset(myds)\n","        if classes == None:\n","            n_classes = len(set(myds.labels))\n","            classes = [i for i in range(n_classes)]\n","        self.my_tasks = l2l.data.TaskDataset(self.myMds, task_transforms=[\n","                                l2l.data.transforms.FilterLabels(self.myMds,classes),\n","                                l2l.data.transforms.NWays(self.myMds,ways),\n","                                l2l.data.transforms.KShots(self.myMds,2*shots),\n","                                l2l.data.transforms.LoadData(self.myMds),\n","                                l2l.data.transforms.RemapLabels(self.myMds),\n","                                l2l.data.transforms.ConsecutiveLabels(self.myMds)\n","                                ],num_tasks=num_tasks)\n","    def get_task(self):\n","        data,labels = self.my_tasks.sample()\n","        adaptation_indices = np.zeros(data.size(0), dtype=bool)\n","        adaptation_indices[np.arange(self.shots*self.ways) * 2] = True\n","        evaluation_indices = torch.from_numpy(~adaptation_indices)\n","        adaptation_indices = torch.from_numpy(adaptation_indices)\n","        adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n","        evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n","        d_train = (adaptation_data,adaptation_labels)\n","        d_test = (evaluation_data,evaluation_labels)\n","        return d_train, d_test"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCzh8pDl_lWj"},"source":["# Pre-trained Models"]},{"cell_type":"code","metadata":{"id":"SRQsztFX_lWj","executionInfo":{"status":"ok","timestamp":1617944721708,"user_tz":-330,"elapsed":8235,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["#Generate data - euclidean\n","meta_train_ds, meta_test_ds, full_loader = utils.euclideanDataset(n_samples=10000,n_features=20,n_classes=10,batch_size=32)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxmAII4a_lWj","executionInfo":{"status":"ok","timestamp":1617944721709,"user_tz":-330,"elapsed":8224,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["# Define an MLP network. Note that input dimension has to be data dimension. For classification\n","# final dimension has to be number of classes; for regression one.\n","#torch.manual_seed(10)\n","net = models.MLP(dims=[20,32,32,10])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkTxepcm_lWk","executionInfo":{"status":"ok","timestamp":1617944724308,"user_tz":-330,"elapsed":10811,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"2062e177-4c7a-4a17-93e6-4dc7709f435c"},"source":["# Train the network; note that network is trained in place so repeated calls further train it.\n","net,loss,accs=models.Train(net,full_loader,lr=1e-2,epochs=5,verbose=True)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch   4 Loss: 1.37655e-01 Accuracy: 0.96995\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7nukjj4_lWk","executionInfo":{"status":"ok","timestamp":1617944724310,"user_tz":-330,"elapsed":10801,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"6b99094c-0e47-4612-f4f4-ff663e5f5069"},"source":["#Training accuracy.\n","models.accuracy(net,meta_train_ds.samples,meta_train_ds.labels,verbose=True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["7287.0 7500\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.9716"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUhTS9XS_lWl","executionInfo":{"status":"ok","timestamp":1617944724311,"user_tz":-330,"elapsed":10792,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"6893be06-ae4f-4308-d034-89f4f56d8e70"},"source":["# Test accuracy.\n","models.accuracy(net,meta_test_ds.samples,meta_test_ds.labels)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2394.0 2500\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.9576"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"-5485YZk_lWl"},"source":["# Second-order Differentiation using Autograd"]},{"cell_type":"markdown","metadata":{"id":"_6QACJ-I_lWl"},"source":["Second-order derivatives as needed for MAML"]},{"cell_type":"code","metadata":{"id":"cr0ZNPTh_lWl","executionInfo":{"status":"ok","timestamp":1617944724312,"user_tz":-330,"elapsed":6477,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["network = (lambda x,w: x@w)\n","loss = torch.nn.MSELoss()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yoPK9uT_lWm","executionInfo":{"status":"ok","timestamp":1617944724313,"user_tz":-330,"elapsed":6442,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["Z=(torch.ones(3,1)).float()\n","z=(torch.ones(3,1)*2).float()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5m_hIOS_lWm","executionInfo":{"status":"ok","timestamp":1617944724314,"user_tz":-330,"elapsed":6432,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["Zt=(torch.ones(3,1)*1.5).float()\n","zt=(torch.ones(3,1)*2*1.5).float()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"pShLEycc_lWm","executionInfo":{"status":"ok","timestamp":1617944724315,"user_tz":-330,"elapsed":6424,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["w0=(torch.ones(1,1,requires_grad=True)).float()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXsMmXpB_lWn","executionInfo":{"status":"ok","timestamp":1617944724315,"user_tz":-330,"elapsed":6400,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["w1=w0.clone()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"va3Rbvxn_lWn","executionInfo":{"status":"ok","timestamp":1617944724316,"user_tz":-330,"elapsed":6388,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["L=loss(network(Z,w1),z)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw14ZTwx_lWo","executionInfo":{"status":"ok","timestamp":1617944724316,"user_tz":-330,"elapsed":6377,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["#g=torch.autograd.grad(L,w0)[0]\n","g=torch.autograd.grad(L,w1,create_graph=True)[0]\n","#L.backward(create_graph=True)# Not good"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGtUzSMU_lWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944724317,"user_tz":-330,"elapsed":6368,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"c0e89210-2008-44dc-e7ec-103a840daeaa"},"source":["w1.grad, w0.grad, L, w0, w1,w1.requires_grad,g"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(None,\n"," None,\n"," tensor(1., grad_fn=<MseLossBackward>),\n"," tensor([[1.]], requires_grad=True),\n"," tensor([[1.]], grad_fn=<CloneBackward>),\n"," True,\n"," tensor([[-2.]], grad_fn=<TBackward>))"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"6yTabNhe_lWp","executionInfo":{"status":"ok","timestamp":1617944724317,"user_tz":-330,"elapsed":6355,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["w1 = w1 - 0.1*g"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZrCzcsND_lWp","executionInfo":{"status":"ok","timestamp":1617944724318,"user_tz":-330,"elapsed":6347,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["L1=loss(network(Zt,w1),zt)\n","#L1=loss(net(Zt,w0-0.1*(2.0*(w0-2.0))),zt)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLmAcnQW_lWp","executionInfo":{"status":"ok","timestamp":1617944724319,"user_tz":-330,"elapsed":6340,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["# Both OK - latter used with optimizer.step()\n","g1=torch.autograd.grad(L1,w0)[0]\n","#L1.backward()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"zW49SMfH_lWq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944724319,"user_tz":-330,"elapsed":6333,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"2220a498-c39f-4650-eac9-d06a6d1ddffa"},"source":["g1"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-2.8800]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"sl8Tg0LF_lWq"},"source":["Working this out manually:"]},{"cell_type":"markdown","metadata":{"id":"2XOlgFMX_lWq"},"source":["$w_0=1, L=(w_0-2)^2, dL=2\\times(w_0-2)=-2,w_1=w_0-0.1\\times(-2)=1.2$"]},{"cell_type":"markdown","metadata":{"id":"fQIvnn5d_lWq"},"source":["$L_1=(w_1\\times1.5-3)^2 = (w_0-0.1\\times(2\\times(w_0-2))\\times1.5-3)^2 = (-1.2)^2$"]},{"cell_type":"markdown","metadata":{"id":"HDFuB8bC_lWq"},"source":["$dL_1 = 2 \\times (-1.2) \\times (1.5 \\times (1-.2)$"]},{"cell_type":"code","metadata":{"id":"bPPSDmxI_lWr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944724320,"user_tz":-330,"elapsed":6326,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"5f43e79f-d0fc-48f0-b704-12dcfe4cf06f"},"source":["2*(-1.2)*(1.5*(1-.2))"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-2.8800000000000003"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"t7zwOnEj_lWr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944724320,"user_tz":-330,"elapsed":6318,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"eab5990b-5ee3-4518-e608-a3ba38ab4ad3"},"source":["w0.grad,w1.grad"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"5eLfCnTA_lWr"},"source":["# Meta-Learning: Tasks"]},{"cell_type":"markdown","metadata":{"id":"9azFb6ia_lWr"},"source":["Generate a k-shot n-way loader using the meta-training dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODgHua2q_lWr","executionInfo":{"status":"ok","timestamp":1617944724321,"user_tz":-330,"elapsed":4462,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"91e060d8-1fcd-4452-df19-6e4759c677c1"},"source":["classes_train = [i for i in range(5)]\n","classes_test = [i+5 for i in range(5)]\n","classes_train, classes_test"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"JHRaczq3_lWr","executionInfo":{"status":"ok","timestamp":1617944724321,"user_tz":-330,"elapsed":4449,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["meta_train_kloader=KShotLoader(meta_train_ds,shots=2,ways=5)"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJnYSOys_lWs"},"source":["Sample a task - each task has a k-shot n-way training set and a similar test set"]},{"cell_type":"code","metadata":{"id":"xaMfqpsy_lWs","executionInfo":{"status":"ok","timestamp":1617944725301,"user_tz":-330,"elapsed":5395,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["d_train,d_test=meta_train_kloader.get_task()"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFYYFwXfKFsp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944725302,"user_tz":-330,"elapsed":5364,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"f61d8604-a053-4b0b-891a-1109463b6202"},"source":["d_test[1]"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 4, 4, 2, 2, 3, 3, 1, 1])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"MbaEKNKS_lWs"},"source":["Let's try directly learning using the task training set albeit its small size: create a dataset and loader and train it with the earlier network and Train function."]},{"cell_type":"code","metadata":{"id":"DjnBl0Lw_lWs","executionInfo":{"status":"ok","timestamp":1617944725304,"user_tz":-330,"elapsed":5357,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["taskds = utils.MyDS(d_train[0],d_train[1])"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hd6LFEmk_lWs","executionInfo":{"status":"ok","timestamp":1617944725306,"user_tz":-330,"elapsed":5351,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["d_train_loader = torch.utils.data.DataLoader(dataset=taskds,batch_size=1,shuffle=True)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qH4qpmO_lWt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944725307,"user_tz":-330,"elapsed":5332,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"018cde1b-622c-4ea9-99d7-531ed6949012"},"source":["net,losses,accs=models.Train(net,d_train_loader,lr=1e-1,epochs=10,verbose=True)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Epoch   9 Loss: 1.13965e-02 Accuracy: 1.00000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j79-xn7n_lWt"},"source":["How does it do on the test set of the sampled task?"]},{"cell_type":"code","metadata":{"id":"hPzyqAnI_lWt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944725310,"user_tz":-330,"elapsed":5327,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"243e3f38-69bd-40dd-b56d-0c408e17a6bd"},"source":["models.accuracy(net,d_test[0],d_test[1])"],"execution_count":34,"outputs":[{"output_type":"stream","text":["8.0 10\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"A2RmBKMp_lWt"},"source":["# MAML - Model-Agnostic Meta-Learning"]},{"cell_type":"code","metadata":{"id":"X8aAOyIG_lWt","executionInfo":{"status":"ok","timestamp":1617944727715,"user_tz":-330,"elapsed":780,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["import learn2learn as l2l\n","import torch.optim as optim"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdzbtEec_lWt","executionInfo":{"status":"ok","timestamp":1617944728122,"user_tz":-330,"elapsed":1176,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["maml = l2l.algorithms.MAML(net, lr=1e-1)\n","optimizer = optim.Adam(net.parameters(),lr=1e-3)\n","lossfn = torch.nn.NLLLoss()"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bykHvBJj_lWu"},"source":["The MAML class above wraps our nn.Module class for parameter cloning and other purposes as below. One iteration of the MAML algorithm proceeds by first sampling a training task: Note that each of d_train and d_test is a tuple comprising of a training set, and labels."]},{"cell_type":"code","metadata":{"id":"2GdlQwtY_lWu","executionInfo":{"status":"ok","timestamp":1617944728122,"user_tz":-330,"elapsed":1167,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["d_train,d_test=meta_train_kloader.get_task()"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSW46vNw_lWu","executionInfo":{"status":"ok","timestamp":1617944728123,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["learner = maml.clone()"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hkd9mq9D_lWv"},"source":["The learner class above is a 'clone' of our network with copies of parameters so that we can change these without changing the parameters of the network. We apply the learner on training data of d_train and compute TRAINING loss w.r.t the training data of the task, i.e., d_train."]},{"cell_type":"code","metadata":{"id":"fgTsiLS4_lWv","executionInfo":{"status":"ok","timestamp":1617944728124,"user_tz":-330,"elapsed":1152,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["train_preds = learner(d_train[0])\n","train_loss = lossfn(train_preds,d_train[1])"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaOoTICh_lWv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728126,"user_tz":-330,"elapsed":1150,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"5216c1dd-9d18-4ed2-d0da-b7a4f2249b8b"},"source":["net.layers[0].weight"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.0170,  0.2050, -0.1767, -0.0742,  0.0689, -0.4018, -0.1227, -0.0731,\n","          0.2030, -0.0670, -0.0181,  0.0241, -0.3519, -0.3282,  0.1709,  0.0905,\n","          0.2800,  0.1334,  0.2410, -0.3506],\n","        [-0.0458, -0.1684,  0.1120,  0.2084, -0.2014,  0.1386,  0.1588,  0.0160,\n","          0.3254, -0.1710,  0.0373,  0.2474,  0.1794,  0.1083, -0.0437,  0.2532,\n","          0.3755, -0.0917,  0.0109, -0.0544],\n","        [ 0.1411, -0.1818, -0.1080, -0.0474,  0.2119,  0.0112, -0.0047, -0.2275,\n","         -0.1094,  0.1886,  0.0798, -0.0432,  0.1764, -0.1920,  0.3518, -0.1561,\n","         -0.3709,  0.0619,  0.0253,  0.0599],\n","        [ 0.1448, -0.2944,  0.2867, -0.1107, -0.1414, -0.0796, -0.1252, -0.1100,\n","         -0.1379, -0.1652,  0.1786,  0.0779,  0.1500, -0.2978, -0.0799,  0.1574,\n","          0.2165, -0.0510, -0.1667,  0.1264],\n","        [ 0.3457, -0.3076,  0.0544,  0.0358,  0.0608,  0.1720, -0.0316, -0.1716,\n","         -0.0502, -0.1075, -0.0986,  0.1823, -0.1532,  0.1865, -0.0009,  0.1407,\n","         -0.2278,  0.0048, -0.0515,  0.2553],\n","        [-0.3077, -0.3734,  0.0265,  0.0166, -0.2168, -0.1102,  0.1960, -0.0742,\n","          0.0966, -0.2835,  0.1617,  0.2094,  0.2351, -0.0189, -0.1587, -0.0033,\n","          0.1815, -0.1292,  0.2455,  0.2612],\n","        [-0.1052, -0.2977,  0.0099, -0.2287,  0.2633, -0.0478,  0.0172, -0.2991,\n","         -0.1248, -0.2609, -0.2294, -0.0307, -0.2048,  0.0688, -0.2215,  0.3749,\n","          0.2954, -0.0755, -0.0575, -0.0230],\n","        [-0.0442,  0.1185,  0.1107,  0.1273,  0.2049, -0.0755,  0.1663, -0.2325,\n","         -0.3030, -0.1796, -0.2871, -0.0570,  0.0423, -0.2100, -0.2447, -0.0910,\n","         -0.0206, -0.1715, -0.1992, -0.0267],\n","        [ 0.0902,  0.0472, -0.3160, -0.1415,  0.1742,  0.2432, -0.2250, -0.1638,\n","          0.0113, -0.2978, -0.3506, -0.2910, -0.2033,  0.2061, -0.0487, -0.0352,\n","          0.0033, -0.1716,  0.0483,  0.2004],\n","        [ 0.0404,  0.2355,  0.2849,  0.0229,  0.0323, -0.1791,  0.0856,  0.1606,\n","         -0.2125,  0.3547,  0.0445,  0.2296,  0.0537,  0.2960,  0.3918, -0.0787,\n","          0.1475, -0.0518,  0.0416,  0.1518],\n","        [ 0.0782, -0.0688,  0.0920, -0.1800,  0.2022,  0.4141,  0.2307,  0.1526,\n","          0.1455,  0.1004, -0.2602,  0.2166, -0.0519,  0.2260,  0.0050,  0.1662,\n","         -0.0647, -0.0545,  0.1151, -0.0273],\n","        [ 0.1576,  0.1372,  0.1569,  0.2114, -0.2161,  0.1308, -0.3092,  0.0373,\n","         -0.1945,  0.0722,  0.1902, -0.0035,  0.0199,  0.1595,  0.2095,  0.0082,\n","         -0.0575, -0.0993,  0.0928, -0.2637],\n","        [ 0.0129, -0.2318, -0.2168, -0.3555,  0.1270,  0.3932, -0.3943, -0.3016,\n","         -0.1280, -0.1802,  0.1098, -0.1888,  0.0201,  0.1098, -0.2436,  0.1297,\n","          0.1769, -0.0612,  0.0132,  0.1107],\n","        [-0.0753, -0.0359,  0.0527,  0.1016,  0.0436,  0.1592,  0.3309,  0.1157,\n","          0.1743,  0.0972,  0.1569, -0.3163, -0.3239, -0.2741, -0.1874, -0.0862,\n","         -0.1588,  0.2344,  0.0491,  0.0085],\n","        [ 0.2900, -0.0171,  0.2807,  0.3238,  0.1202,  0.1653, -0.0174,  0.1782,\n","         -0.1179, -0.2273, -0.1659,  0.0995,  0.0736, -0.1137, -0.0800,  0.1449,\n","          0.2323,  0.2318,  0.1771,  0.2517],\n","        [-0.1874,  0.1917,  0.2312, -0.2189,  0.1224, -0.1517, -0.1451,  0.1384,\n","          0.1132,  0.2667,  0.1131,  0.2987, -0.1382,  0.1905,  0.0193,  0.0669,\n","          0.1999, -0.1100,  0.2144, -0.2683],\n","        [-0.1835,  0.0546,  0.2354, -0.0260,  0.3272,  0.0147,  0.2160,  0.1061,\n","         -0.0710,  0.2005,  0.4113,  0.2566,  0.3505,  0.0819,  0.2056,  0.0105,\n","          0.1755,  0.2836, -0.0270,  0.0238],\n","        [-0.1077,  0.2402,  0.1321, -0.2740,  0.0684, -0.3714,  0.1483,  0.2870,\n","         -0.1754, -0.1053,  0.2241,  0.0457,  0.0190, -0.1261,  0.1902,  0.0708,\n","         -0.1002,  0.3626, -0.0218,  0.3320],\n","        [ 0.0559, -0.3088,  0.1835, -0.3155,  0.1872,  0.0301,  0.1621,  0.0334,\n","          0.3100, -0.3799, -0.1036,  0.1419,  0.4258, -0.0140,  0.3285, -0.0288,\n","          0.1095, -0.0525,  0.0975, -0.2287],\n","        [-0.1448, -0.1499, -0.0123, -0.2117,  0.1930, -0.1203,  0.1406,  0.0582,\n","         -0.1897,  0.2967,  0.2234,  0.0674, -0.1676, -0.0448, -0.1352, -0.1358,\n","          0.1781,  0.3492,  0.2709, -0.0343],\n","        [-0.3924,  0.2585, -0.0706, -0.2910,  0.0758, -0.3450, -0.1469,  0.0035,\n","          0.1934, -0.2827, -0.1505,  0.0253, -0.2689, -0.1584,  0.0702,  0.0277,\n","          0.0812,  0.0316, -0.0401, -0.4126],\n","        [ 0.0163,  0.0352, -0.2961,  0.0114,  0.1122, -0.0632,  0.2209,  0.3072,\n","          0.0123,  0.1604, -0.1344,  0.1235, -0.4639, -0.0142, -0.0731,  0.1645,\n","         -0.0236, -0.0862, -0.0969,  0.0466],\n","        [ 0.2669,  0.2683,  0.3021,  0.3215, -0.1360,  0.3846, -0.1364,  0.0733,\n","          0.0774,  0.0963,  0.0814, -0.2383,  0.0080,  0.1744,  0.0551, -0.0040,\n","          0.0910, -0.2031, -0.0930, -0.1752],\n","        [-0.0545, -0.0370,  0.1228, -0.1380, -0.2417, -0.1100,  0.1892,  0.1400,\n","         -0.0354,  0.0033, -0.2500, -0.0560, -0.0958,  0.2113,  0.2557, -0.3500,\n","          0.1511, -0.2534, -0.0578, -0.0977],\n","        [-0.0571,  0.2600,  0.1747,  0.1952,  0.0211,  0.3074,  0.0904,  0.3016,\n","          0.0291, -0.2095,  0.2838,  0.0745,  0.2490, -0.1696,  0.0049,  0.0201,\n","          0.0452,  0.3590, -0.1355,  0.1791],\n","        [ 0.0958, -0.1032,  0.2810, -0.1131,  0.0846, -0.0584, -0.2567, -0.2705,\n","          0.0350,  0.1679,  0.0917,  0.1414, -0.2533,  0.1840, -0.0248,  0.0064,\n","         -0.2898, -0.3404,  0.0016, -0.0488],\n","        [-0.2654, -0.0694,  0.1377,  0.2502, -0.1990, -0.1429, -0.0681, -0.2051,\n","          0.0589, -0.1574,  0.1444,  0.2358,  0.0019, -0.1988, -0.3993, -0.0284,\n","         -0.0686, -0.0873,  0.0875,  0.0025],\n","        [ 0.2160,  0.2656, -0.0563, -0.0892, -0.2037, -0.2095, -0.1104, -0.1373,\n","          0.1480,  0.1868, -0.2485, -0.0173, -0.2133,  0.2651,  0.1378, -0.0710,\n","          0.1196,  0.0668,  0.2720, -0.1050],\n","        [ 0.0168, -0.0516, -0.2534, -0.0026, -0.1156,  0.1143,  0.0430,  0.3583,\n","         -0.1680,  0.0239,  0.2241, -0.2012, -0.1346, -0.0548, -0.0571, -0.1166,\n","         -0.3035,  0.0562,  0.2836,  0.0671],\n","        [ 0.2273,  0.2177,  0.0507,  0.2881, -0.2174,  0.1839,  0.1305,  0.1261,\n","          0.1289, -0.1059,  0.1073,  0.1214, -0.1270,  0.0057, -0.3152, -0.3482,\n","          0.0191,  0.2807, -0.1342, -0.0308],\n","        [-0.3013,  0.1698, -0.0607,  0.0218, -0.0151, -0.1095,  0.2463, -0.0186,\n","         -0.3514, -0.1125, -0.0166, -0.0210, -0.0566, -0.1975, -0.2915,  0.0518,\n","         -0.1442,  0.1524,  0.2290, -0.0116],\n","        [ 0.1645,  0.0240,  0.0854,  0.3921, -0.2626,  0.2181,  0.1251, -0.1379,\n","          0.0278,  0.0854, -0.0611, -0.2527, -0.2670,  0.0143, -0.0624, -0.2151,\n","         -0.0501, -0.1677,  0.1106, -0.1448]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"zH-z32ZE_lWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728937,"user_tz":-330,"elapsed":1954,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"80ca5776-59b1-4dab-dad6-fc5a8b589fe7"},"source":["learner.layers[0].weight"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0170,  0.2050, -0.1767, -0.0742,  0.0689, -0.4018, -0.1227, -0.0731,\n","          0.2030, -0.0670, -0.0181,  0.0241, -0.3519, -0.3282,  0.1709,  0.0905,\n","          0.2800,  0.1334,  0.2410, -0.3506],\n","        [-0.0458, -0.1684,  0.1120,  0.2084, -0.2014,  0.1386,  0.1588,  0.0160,\n","          0.3254, -0.1710,  0.0373,  0.2474,  0.1794,  0.1083, -0.0437,  0.2532,\n","          0.3755, -0.0917,  0.0109, -0.0544],\n","        [ 0.1411, -0.1818, -0.1080, -0.0474,  0.2119,  0.0112, -0.0047, -0.2275,\n","         -0.1094,  0.1886,  0.0798, -0.0432,  0.1764, -0.1920,  0.3518, -0.1561,\n","         -0.3709,  0.0619,  0.0253,  0.0599],\n","        [ 0.1448, -0.2944,  0.2867, -0.1107, -0.1414, -0.0796, -0.1252, -0.1100,\n","         -0.1379, -0.1652,  0.1786,  0.0779,  0.1500, -0.2978, -0.0799,  0.1574,\n","          0.2165, -0.0510, -0.1667,  0.1264],\n","        [ 0.3457, -0.3076,  0.0544,  0.0358,  0.0608,  0.1720, -0.0316, -0.1716,\n","         -0.0502, -0.1075, -0.0986,  0.1823, -0.1532,  0.1865, -0.0009,  0.1407,\n","         -0.2278,  0.0048, -0.0515,  0.2553],\n","        [-0.3077, -0.3734,  0.0265,  0.0166, -0.2168, -0.1102,  0.1960, -0.0742,\n","          0.0966, -0.2835,  0.1617,  0.2094,  0.2351, -0.0189, -0.1587, -0.0033,\n","          0.1815, -0.1292,  0.2455,  0.2612],\n","        [-0.1052, -0.2977,  0.0099, -0.2287,  0.2633, -0.0478,  0.0172, -0.2991,\n","         -0.1248, -0.2609, -0.2294, -0.0307, -0.2048,  0.0688, -0.2215,  0.3749,\n","          0.2954, -0.0755, -0.0575, -0.0230],\n","        [-0.0442,  0.1185,  0.1107,  0.1273,  0.2049, -0.0755,  0.1663, -0.2325,\n","         -0.3030, -0.1796, -0.2871, -0.0570,  0.0423, -0.2100, -0.2447, -0.0910,\n","         -0.0206, -0.1715, -0.1992, -0.0267],\n","        [ 0.0902,  0.0472, -0.3160, -0.1415,  0.1742,  0.2432, -0.2250, -0.1638,\n","          0.0113, -0.2978, -0.3506, -0.2910, -0.2033,  0.2061, -0.0487, -0.0352,\n","          0.0033, -0.1716,  0.0483,  0.2004],\n","        [ 0.0404,  0.2355,  0.2849,  0.0229,  0.0323, -0.1791,  0.0856,  0.1606,\n","         -0.2125,  0.3547,  0.0445,  0.2296,  0.0537,  0.2960,  0.3918, -0.0787,\n","          0.1475, -0.0518,  0.0416,  0.1518],\n","        [ 0.0782, -0.0688,  0.0920, -0.1800,  0.2022,  0.4141,  0.2307,  0.1526,\n","          0.1455,  0.1004, -0.2602,  0.2166, -0.0519,  0.2260,  0.0050,  0.1662,\n","         -0.0647, -0.0545,  0.1151, -0.0273],\n","        [ 0.1576,  0.1372,  0.1569,  0.2114, -0.2161,  0.1308, -0.3092,  0.0373,\n","         -0.1945,  0.0722,  0.1902, -0.0035,  0.0199,  0.1595,  0.2095,  0.0082,\n","         -0.0575, -0.0993,  0.0928, -0.2637],\n","        [ 0.0129, -0.2318, -0.2168, -0.3555,  0.1270,  0.3932, -0.3943, -0.3016,\n","         -0.1280, -0.1802,  0.1098, -0.1888,  0.0201,  0.1098, -0.2436,  0.1297,\n","          0.1769, -0.0612,  0.0132,  0.1107],\n","        [-0.0753, -0.0359,  0.0527,  0.1016,  0.0436,  0.1592,  0.3309,  0.1157,\n","          0.1743,  0.0972,  0.1569, -0.3163, -0.3239, -0.2741, -0.1874, -0.0862,\n","         -0.1588,  0.2344,  0.0491,  0.0085],\n","        [ 0.2900, -0.0171,  0.2807,  0.3238,  0.1202,  0.1653, -0.0174,  0.1782,\n","         -0.1179, -0.2273, -0.1659,  0.0995,  0.0736, -0.1137, -0.0800,  0.1449,\n","          0.2323,  0.2318,  0.1771,  0.2517],\n","        [-0.1874,  0.1917,  0.2312, -0.2189,  0.1224, -0.1517, -0.1451,  0.1384,\n","          0.1132,  0.2667,  0.1131,  0.2987, -0.1382,  0.1905,  0.0193,  0.0669,\n","          0.1999, -0.1100,  0.2144, -0.2683],\n","        [-0.1835,  0.0546,  0.2354, -0.0260,  0.3272,  0.0147,  0.2160,  0.1061,\n","         -0.0710,  0.2005,  0.4113,  0.2566,  0.3505,  0.0819,  0.2056,  0.0105,\n","          0.1755,  0.2836, -0.0270,  0.0238],\n","        [-0.1077,  0.2402,  0.1321, -0.2740,  0.0684, -0.3714,  0.1483,  0.2870,\n","         -0.1754, -0.1053,  0.2241,  0.0457,  0.0190, -0.1261,  0.1902,  0.0708,\n","         -0.1002,  0.3626, -0.0218,  0.3320],\n","        [ 0.0559, -0.3088,  0.1835, -0.3155,  0.1872,  0.0301,  0.1621,  0.0334,\n","          0.3100, -0.3799, -0.1036,  0.1419,  0.4258, -0.0140,  0.3285, -0.0288,\n","          0.1095, -0.0525,  0.0975, -0.2287],\n","        [-0.1448, -0.1499, -0.0123, -0.2117,  0.1930, -0.1203,  0.1406,  0.0582,\n","         -0.1897,  0.2967,  0.2234,  0.0674, -0.1676, -0.0448, -0.1352, -0.1358,\n","          0.1781,  0.3492,  0.2709, -0.0343],\n","        [-0.3924,  0.2585, -0.0706, -0.2910,  0.0758, -0.3450, -0.1469,  0.0035,\n","          0.1934, -0.2827, -0.1505,  0.0253, -0.2689, -0.1584,  0.0702,  0.0277,\n","          0.0812,  0.0316, -0.0401, -0.4126],\n","        [ 0.0163,  0.0352, -0.2961,  0.0114,  0.1122, -0.0632,  0.2209,  0.3072,\n","          0.0123,  0.1604, -0.1344,  0.1235, -0.4639, -0.0142, -0.0731,  0.1645,\n","         -0.0236, -0.0862, -0.0969,  0.0466],\n","        [ 0.2669,  0.2683,  0.3021,  0.3215, -0.1360,  0.3846, -0.1364,  0.0733,\n","          0.0774,  0.0963,  0.0814, -0.2383,  0.0080,  0.1744,  0.0551, -0.0040,\n","          0.0910, -0.2031, -0.0930, -0.1752],\n","        [-0.0545, -0.0370,  0.1228, -0.1380, -0.2417, -0.1100,  0.1892,  0.1400,\n","         -0.0354,  0.0033, -0.2500, -0.0560, -0.0958,  0.2113,  0.2557, -0.3500,\n","          0.1511, -0.2534, -0.0578, -0.0977],\n","        [-0.0571,  0.2600,  0.1747,  0.1952,  0.0211,  0.3074,  0.0904,  0.3016,\n","          0.0291, -0.2095,  0.2838,  0.0745,  0.2490, -0.1696,  0.0049,  0.0201,\n","          0.0452,  0.3590, -0.1355,  0.1791],\n","        [ 0.0958, -0.1032,  0.2810, -0.1131,  0.0846, -0.0584, -0.2567, -0.2705,\n","          0.0350,  0.1679,  0.0917,  0.1414, -0.2533,  0.1840, -0.0248,  0.0064,\n","         -0.2898, -0.3404,  0.0016, -0.0488],\n","        [-0.2654, -0.0694,  0.1377,  0.2502, -0.1990, -0.1429, -0.0681, -0.2051,\n","          0.0589, -0.1574,  0.1444,  0.2358,  0.0019, -0.1988, -0.3993, -0.0284,\n","         -0.0686, -0.0873,  0.0875,  0.0025],\n","        [ 0.2160,  0.2656, -0.0563, -0.0892, -0.2037, -0.2095, -0.1104, -0.1373,\n","          0.1480,  0.1868, -0.2485, -0.0173, -0.2133,  0.2651,  0.1378, -0.0710,\n","          0.1196,  0.0668,  0.2720, -0.1050],\n","        [ 0.0168, -0.0516, -0.2534, -0.0026, -0.1156,  0.1143,  0.0430,  0.3583,\n","         -0.1680,  0.0239,  0.2241, -0.2012, -0.1346, -0.0548, -0.0571, -0.1166,\n","         -0.3035,  0.0562,  0.2836,  0.0671],\n","        [ 0.2273,  0.2177,  0.0507,  0.2881, -0.2174,  0.1839,  0.1305,  0.1261,\n","          0.1289, -0.1059,  0.1073,  0.1214, -0.1270,  0.0057, -0.3152, -0.3482,\n","          0.0191,  0.2807, -0.1342, -0.0308],\n","        [-0.3013,  0.1698, -0.0607,  0.0218, -0.0151, -0.1095,  0.2463, -0.0186,\n","         -0.3514, -0.1125, -0.0166, -0.0210, -0.0566, -0.1975, -0.2915,  0.0518,\n","         -0.1442,  0.1524,  0.2290, -0.0116],\n","        [ 0.1645,  0.0240,  0.0854,  0.3921, -0.2626,  0.2181,  0.1251, -0.1379,\n","          0.0278,  0.0854, -0.0611, -0.2527, -0.2670,  0.0143, -0.0624, -0.2151,\n","         -0.0501, -0.1677,  0.1106, -0.1448]], grad_fn=<CloneBackward>)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"jFAxTv3u_lWw"},"source":["Note that at this point both the learner and original net have the same parameters. Lets see what the gradients w.r.t the TRAINING loss are: (We use pytorch's autograd functions directly.)"]},{"cell_type":"code","metadata":{"id":"BIBHhQeT_lWw","executionInfo":{"status":"ok","timestamp":1617944728938,"user_tz":-330,"elapsed":1945,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["from torch.autograd import grad"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkAxGBJ4_lWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728939,"user_tz":-330,"elapsed":1942,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"924c7443-2018-4593-9302-bc8513922769"},"source":["train_grad=grad(train_loss,learner.layers[0].weight,retain_graph=True,\n","                                 create_graph=True,\n","                                 allow_unused=True)\n","train_grad[0]"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.5369e-01,  7.6493e-03,  5.1484e-02, -4.8598e-03, -2.5352e-01,\n","         -1.6761e-01,  1.7654e-02,  4.2795e-02,  1.6681e-01,  6.7950e-02,\n","         -6.7698e-02,  5.0039e-02, -6.7161e-02,  1.2873e-01, -3.8113e-02,\n","          1.0247e-02,  7.2580e-03, -1.1101e-01,  1.0865e-02,  2.7140e-02],\n","        [-2.5535e-02, -3.3880e-02, -3.9947e-03,  2.8470e-02, -4.4797e-02,\n","         -9.4264e-02,  6.8652e-03, -8.0587e-02,  5.1062e-02,  1.5032e-02,\n","          7.1067e-02, -4.2555e-03,  1.9587e-02,  2.0244e-02, -1.1713e-02,\n","          3.8650e-02,  9.0536e-03, -4.4504e-02,  1.3211e-02, -4.8954e-03],\n","        [-3.0367e-02, -3.7475e-01, -2.0228e-01, -4.7997e-01,  2.2323e-01,\n","         -2.3587e-01,  3.0371e-01, -2.5351e-01, -3.3591e-02, -4.4211e-02,\n","          5.5854e-03,  5.6466e-02,  1.8287e-01, -3.7727e-02, -1.7420e-01,\n","          1.0689e-02,  5.1989e-02,  3.5636e-01, -3.1156e-02,  6.3373e-02],\n","        [ 2.1314e-01, -7.2168e-03, -1.1497e-01,  6.7505e-02,  2.5608e-01,\n","          2.1217e-01, -8.3638e-02, -1.3606e-01, -2.0268e-01, -6.5542e-02,\n","          2.1319e-01, -8.8564e-02,  2.3732e-01, -2.5007e-01,  6.1175e-02,\n","          8.0479e-02, -1.8915e-02,  1.0483e-01,  6.8081e-02, -8.7479e-02],\n","        [ 4.2934e-01, -4.9136e-02, -1.9883e-01, -9.2129e-02,  3.9196e-01,\n","          1.4528e-01, -9.4734e-02, -3.4241e-01, -2.4375e-01,  1.4154e-02,\n","          2.8921e-01, -1.9914e-01,  2.0750e-01, -2.3969e-01, -1.1362e-02,\n","          9.9206e-02, -1.3955e-01,  1.1557e-01,  2.6861e-01,  1.3209e-01],\n","        [-1.6170e-01, -2.4375e-01,  9.1550e-03,  8.2863e-02, -1.4572e-01,\n","         -5.7563e-01,  1.4197e-01, -5.0672e-01,  2.3386e-01,  9.8491e-02,\n","          3.9786e-01, -5.5010e-02,  7.9303e-02,  1.3411e-01, -7.1731e-02,\n","          2.0076e-01,  4.6515e-02, -1.6155e-01,  7.7052e-02,  4.5305e-02],\n","        [ 4.4738e-02, -3.7620e-01, -1.2381e-02, -4.0390e-02,  4.2998e-01,\n","         -5.3584e-01,  3.3212e-01, -7.6574e-01, -3.1690e-02, -2.4973e-02,\n","          4.8573e-01, -1.7609e-01, -7.3958e-04,  1.1346e-01, -6.5788e-02,\n","          1.1750e-01,  7.3989e-02,  9.7725e-02, -7.3803e-02,  1.2354e-01],\n","        [ 1.2941e-01,  9.5646e-02, -2.7424e-02,  1.5914e-01,  4.5284e-02,\n","          1.9870e-01, -1.5159e-01, -4.3259e-03, -1.2036e-01, -8.4410e-03,\n","          1.2982e-01, -6.7588e-02,  1.5659e-01, -1.7500e-01,  7.3780e-02,\n","          4.3418e-02, -2.9389e-02, -3.2945e-02,  8.2484e-02, -7.0181e-02],\n","        [ 6.3584e-01,  2.5460e-01, -9.6740e-02,  1.8649e-01,  4.6585e-01,\n","          3.8524e-01, -2.7054e-01, -4.1367e-01, -4.0106e-01,  5.7449e-02,\n","          4.0151e-01, -4.1667e-01,  1.5805e-01, -2.7824e-01,  1.3048e-01,\n","          1.4196e-01, -2.3231e-01, -2.1327e-02,  3.5660e-01,  1.7029e-01],\n","        [ 1.7225e-01,  3.1131e-01,  2.1539e-01,  6.2028e-01, -9.4095e-02,\n","          1.2694e-01, -3.5616e-01,  3.7479e-02, -5.6709e-02, -2.2912e-02,\n","          1.4494e-01, -1.2109e-01, -1.9486e-01,  5.0030e-02,  1.4286e-01,\n","         -1.7157e-01,  2.4962e-02, -4.5665e-01, -1.2830e-01, -6.8687e-02],\n","        [ 4.0575e-02,  2.2997e-02, -2.1046e-02,  3.4120e-02, -1.4438e-02,\n","          2.5891e-02, -5.3625e-02,  6.6244e-03,  8.9628e-03, -1.1408e-02,\n","          1.1904e-02,  3.5367e-03,  4.2373e-03, -1.2960e-02,  3.1954e-03,\n","          1.9977e-03, -4.3149e-03, -3.8630e-02,  1.9683e-03, -2.2514e-02],\n","        [ 4.7851e-01,  2.9403e-01,  1.7071e-02,  1.7248e-01,  3.1317e-01,\n","          4.5894e-01, -2.6385e-01, -1.1439e-01, -4.3720e-01,  6.3949e-02,\n","          1.5761e-01, -3.2728e-01,  1.2462e-01, -2.5894e-01,  1.1647e-01,\n","         -7.1037e-02, -1.7206e-01, -7.6615e-03,  2.3916e-01,  1.6435e-01],\n","        [ 8.8215e-01, -7.0426e-02, -3.5464e-01, -2.0074e-01,  9.0719e-01,\n","          1.1156e-01, -3.5057e-02, -9.6031e-01, -4.8366e-01, -7.8852e-02,\n","          5.3624e-01, -4.8108e-01,  2.0801e-01, -2.3197e-01, -4.6686e-02,\n","          1.2399e-01, -1.7444e-01,  2.3980e-01,  1.9302e-01,  2.3612e-01],\n","        [-1.9869e-01,  1.4330e-01,  2.2170e-01,  9.5599e-03, -2.3099e-01,\n","         -8.7439e-02,  6.6501e-02,  1.7757e-01,  1.0473e-01,  1.3873e-01,\n","         -2.1416e-01, -1.5214e-02, -2.6769e-01,  2.3104e-01,  1.2974e-02,\n","         -6.7420e-02, -4.7065e-02, -1.0901e-01,  3.3892e-02,  1.7256e-01],\n","        [ 3.6724e-01,  6.8856e-02, -3.8582e-01, -2.2285e-01,  2.7789e-02,\n","          2.5971e-01, -2.1747e-01, -1.2103e-01, -4.9329e-02, -4.4657e-02,\n","          4.5843e-02, -3.4708e-02,  3.3250e-01, -2.4909e-01, -5.8427e-02,\n","          1.8676e-01, -1.1446e-01,  9.7402e-02,  2.0300e-01, -9.4903e-02],\n","        [-1.7619e-02, -8.8424e-02, -2.6005e-01, -1.8021e-01, -2.5392e-01,\n","         -6.9445e-02, -6.6916e-02, -2.8527e-03,  1.6462e-01, -9.5480e-02,\n","         -8.5222e-02,  1.6500e-01,  2.3532e-01, -4.9815e-02, -1.2523e-01,\n","          3.8544e-02,  8.6531e-02,  1.6882e-02, -1.1557e-01, -2.2338e-01],\n","        [ 8.4089e-03,  1.5336e-01, -4.4611e-02,  3.0188e-01, -3.8950e-01,\n","          2.1667e-02, -3.2858e-01,  2.1437e-01,  2.5787e-01, -7.0201e-02,\n","         -4.1386e-02,  1.5738e-01, -6.7988e-02,  5.3814e-02,  1.3392e-02,\n","         -1.5507e-02,  5.2583e-02, -3.7672e-01, -1.1109e-01, -2.3683e-01],\n","        [ 2.8745e-02, -1.7234e-03,  8.1625e-03,  1.3787e-01, -4.7591e-02,\n","         -1.4566e-01, -6.8867e-02, -1.5715e-02,  1.5773e-01, -1.0515e-01,\n","         -2.5548e-02,  8.0369e-02, -2.3486e-01,  1.8322e-01, -2.7694e-02,\n","         -9.1944e-02,  8.2433e-02, -2.0117e-01, -2.3575e-01, -8.7768e-02],\n","        [ 2.1271e-01, -5.8547e-02, -8.4541e-02,  1.8443e-01,  2.3888e-01,\n","         -1.6849e-02, -9.3880e-02, -2.0452e-01, -2.7823e-02, -1.6130e-01,\n","          2.0742e-01, -1.3088e-02, -3.9329e-02, -2.1917e-02,  1.8790e-02,\n","         -1.5171e-02,  7.0664e-02, -8.8469e-02, -1.7752e-01, -1.3673e-01],\n","        [-2.0857e-01, -2.8450e-02,  1.2420e-01, -1.4105e-01, -1.4929e-01,\n","         -1.9831e-01,  1.6022e-01,  9.0118e-02,  1.2475e-01,  8.3677e-02,\n","         -1.9897e-01,  4.6542e-02, -2.0731e-01,  2.1967e-01, -5.9535e-02,\n","         -7.4916e-02,  1.8861e-03, -2.6516e-03, -2.9825e-02,  1.4567e-01],\n","        [ 3.1271e-02, -1.4425e-01, -3.2168e-03,  4.9224e-02,  1.4561e-01,\n","         -2.4896e-01,  8.8734e-02, -2.8797e-01,  4.6738e-02, -4.9839e-02,\n","          1.8401e-01, -3.3257e-02, -8.2016e-02,  1.0058e-01, -3.1488e-02,\n","          1.3197e-02,  5.7654e-02, -4.4653e-02, -1.0864e-01,  8.2476e-03],\n","        [-4.1580e-02,  1.4297e-01,  1.3742e-01,  1.1411e-01, -6.0758e-02,\n","          1.1959e-01, -4.4772e-02,  1.2321e-01, -5.5003e-02,  1.1012e-01,\n","         -7.8376e-03, -7.4451e-02, -3.4229e-02, -2.3957e-02,  8.3187e-02,\n","          2.3840e-02, -8.0774e-02, -5.0709e-02,  1.6108e-01,  9.5276e-02],\n","        [ 3.8160e-01,  3.4135e-01,  2.4712e-01,  4.6652e-01,  2.0692e-01,\n","          2.4319e-01, -2.6480e-01, -2.6831e-01, -4.2389e-01,  1.5114e-01,\n","          3.1971e-01, -4.4129e-01,  1.2107e-02, -1.2645e-01,  1.5878e-01,\n","         -1.4631e-01, -1.3499e-01, -2.3734e-01,  1.9945e-01,  2.4842e-01],\n","        [ 5.0162e-02,  2.0736e-01,  3.0811e-01,  3.9573e-01,  2.7605e-02,\n","          2.3150e-02, -9.9527e-02, -5.2486e-02, -1.6100e-01,  9.1681e-02,\n","          1.1643e-01, -2.0679e-01, -1.8764e-01,  8.0592e-02,  1.1433e-01,\n","         -1.7391e-01, -1.7800e-02, -2.4191e-01, -1.5100e-02,  1.4868e-01],\n","        [ 4.5605e-02,  1.6858e-01,  2.4694e-01,  3.0080e-01,  3.6653e-02,\n","          2.5870e-02, -6.6370e-02, -6.4125e-02, -1.5429e-01,  8.0411e-02,\n","          9.8146e-02, -1.8518e-01, -1.2813e-01,  5.3488e-02,  9.1688e-02,\n","         -1.3783e-01, -1.7855e-02, -1.7072e-01, -3.8606e-03,  1.3015e-01],\n","        [ 1.5478e-01,  1.0691e-01, -2.3633e-01, -1.9684e-01, -1.5269e-01,\n","          2.0254e-01, -1.6113e-01,  7.6849e-02,  3.7523e-02,  2.3821e-02,\n","         -9.7551e-02,  2.3278e-02,  2.0428e-01, -1.3798e-01, -4.4823e-02,\n","          1.3560e-01, -1.0585e-01,  4.5875e-02,  1.9551e-01, -4.5464e-02],\n","        [-5.5690e-02,  9.6981e-02,  1.1163e-01,  1.2880e-01, -1.9121e-01,\n","          1.2570e-02, -8.4692e-02,  6.4108e-02, -4.3979e-02,  8.4910e-02,\n","         -1.0291e-02, -4.7374e-02,  5.7696e-02, -1.7851e-02,  1.3339e-02,\n","         -9.9804e-02,  1.3815e-03, -1.1644e-01,  3.7262e-02,  4.0445e-02],\n","        [-5.5791e-02, -1.7120e-01, -2.0991e-01, -2.0668e-01, -1.8395e-02,\n","         -4.5300e-02,  5.4160e-02,  4.8476e-02,  1.6001e-01, -8.9540e-02,\n","         -3.8482e-02,  1.8008e-01,  1.0107e-01, -4.7172e-02, -6.5205e-02,\n","          1.4196e-01,  3.2408e-02,  1.2398e-01, -1.5106e-02, -1.4901e-01],\n","        [-2.4634e-01,  9.5825e-02,  4.8583e-01,  3.9295e-02,  2.4050e-01,\n","         -1.2688e-01,  3.9348e-01, -1.0166e-01, -1.9559e-01,  2.3171e-01,\n","         -1.4509e-02, -2.8533e-01, -3.6741e-01,  2.5342e-01,  1.3142e-01,\n","         -3.9207e-02, -9.1455e-02,  1.2275e-01,  1.0740e-01,  4.3142e-01],\n","        [ 7.6765e-03,  6.4361e-02,  1.0971e-01,  1.2510e-01, -4.4895e-03,\n","         -4.8847e-03, -2.7363e-02, -2.3065e-02, -6.2676e-02,  3.6689e-02,\n","          3.2881e-02, -7.2746e-02, -5.0520e-02,  2.7833e-02,  2.9796e-02,\n","         -7.8136e-02,  6.4963e-04, -7.8875e-02, -1.2345e-02,  5.7205e-02],\n","        [-3.4035e-01, -9.6921e-02,  1.7781e-01, -4.7744e-02, -1.4249e-01,\n","         -2.4390e-01,  2.1544e-01,  1.5030e-01,  1.5824e-01,  1.9773e-02,\n","         -1.8252e-01,  1.2435e-01, -2.2552e-01,  2.3153e-01, -3.0598e-02,\n","         -9.4444e-02,  9.1081e-02,  8.4643e-04, -1.6792e-01,  3.5520e-02],\n","        [ 1.1722e-01,  2.3015e-01,  2.3329e-01,  2.6482e-01,  5.2642e-02,\n","          1.1558e-01, -1.0790e-01, -2.3778e-02, -1.9995e-01,  9.3694e-02,\n","          4.3201e-02, -2.1221e-01, -1.3738e-01,  3.5471e-02,  9.1797e-02,\n","         -1.6211e-01, -5.7060e-02, -1.5859e-01,  3.5989e-02,  1.6957e-01]],\n","       grad_fn=<TBackward>)"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"NPFfsQAj_lWx"},"source":["Next we ADAPT the learner by taking one step on the CLONED parameters in direction of the gradient of the TRAINING loss above. This is the part that the l2l libarary does for us as per the MAML algorithm."]},{"cell_type":"code","metadata":{"id":"V3IfWLc0_lWx","executionInfo":{"status":"ok","timestamp":1617944728940,"user_tz":-330,"elapsed":1936,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["learner.adapt(train_loss)"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGcs20lg_lWx"},"source":["We can check what has happended:"]},{"cell_type":"code","metadata":{"id":"WlIIQamw_lWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728940,"user_tz":-330,"elapsed":1932,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"3d56bfe6-6ea0-4480-db2d-cf2070a9eca1"},"source":["learner.layers[0].weight"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.6563e-03,  2.0427e-01, -1.8190e-01, -7.3707e-02,  9.4210e-02,\n","         -3.8500e-01, -1.2447e-01, -7.7377e-02,  1.8631e-01, -7.3786e-02,\n","         -1.1373e-02,  1.9119e-02, -3.4514e-01, -3.4105e-01,  1.7467e-01,\n","          8.9478e-02,  2.7923e-01,  1.4451e-01,  2.3991e-01, -3.5328e-01],\n","        [-4.3255e-02, -1.6501e-01,  1.1236e-01,  2.0553e-01, -1.9695e-01,\n","          1.4800e-01,  1.5808e-01,  2.4056e-02,  3.2032e-01, -1.7247e-01,\n","          3.0225e-02,  2.4786e-01,  1.7745e-01,  1.0623e-01, -4.2480e-02,\n","          2.4931e-01,  3.7455e-01, -8.7212e-02,  9.5304e-03, -5.3922e-02],\n","        [ 1.4414e-01, -1.4435e-01, -8.7815e-02,  6.0119e-04,  1.8960e-01,\n","          3.4824e-02, -3.5071e-02, -2.0214e-01, -1.0607e-01,  1.9299e-01,\n","          7.9197e-02, -4.8808e-02,  1.5816e-01, -1.8823e-01,  3.6926e-01,\n","         -1.5719e-01, -3.7613e-01,  2.6285e-02,  2.8384e-02,  5.3519e-02],\n","        [ 1.2346e-01, -2.9373e-01,  2.9821e-01, -1.1749e-01, -1.6697e-01,\n","         -1.0081e-01, -1.1680e-01, -9.6419e-02, -1.1765e-01, -1.5868e-01,\n","          1.5724e-01,  8.6786e-02,  1.2630e-01, -2.7276e-01, -8.6033e-02,\n","          1.4938e-01,  2.1839e-01, -6.1493e-02, -1.7348e-01,  1.3519e-01],\n","        [ 3.0276e-01, -3.0270e-01,  7.4281e-02,  4.4970e-02,  2.1629e-02,\n","          1.5745e-01, -2.2146e-02, -1.3732e-01, -2.5835e-02, -1.0891e-01,\n","         -1.2751e-01,  2.0221e-01, -1.7393e-01,  2.1051e-01,  2.3822e-04,\n","          1.3081e-01, -2.1389e-01, -6.7812e-03, -7.8390e-02,  2.4206e-01],\n","        [-2.9150e-01, -3.4903e-01,  2.5585e-02,  8.2696e-03, -2.0223e-01,\n","         -5.2627e-02,  1.8179e-01, -2.3525e-02,  7.3250e-02, -2.9338e-01,\n","          1.2189e-01,  2.1489e-01,  2.2722e-01, -3.2270e-02, -1.5153e-01,\n","         -2.3396e-02,  1.7686e-01, -1.1302e-01,  2.3780e-01,  2.5663e-01],\n","        [-1.0971e-01, -2.6005e-01,  1.1107e-02, -2.2469e-01,  2.2031e-01,\n","          5.8109e-03, -1.5963e-02, -2.2256e-01, -1.2159e-01, -2.5838e-01,\n","         -2.7796e-01, -1.3082e-02, -2.0470e-01,  5.7455e-02, -2.1489e-01,\n","          3.6319e-01,  2.8798e-01, -8.5279e-02, -5.0074e-02, -3.5353e-02],\n","        [-5.7135e-02,  1.0890e-01,  1.1345e-01,  1.1139e-01,  2.0038e-01,\n","         -9.5335e-02,  1.8145e-01, -2.3209e-01, -2.9101e-01, -1.7871e-01,\n","         -3.0013e-01, -5.0221e-02,  2.6618e-02, -1.9254e-01, -2.5203e-01,\n","         -9.5350e-02, -1.7659e-02, -1.6820e-01, -2.0742e-01, -1.9658e-02],\n","        [ 2.6627e-02,  2.1716e-02, -3.0632e-01, -1.6010e-01,  1.2759e-01,\n","          2.0468e-01, -1.9795e-01, -1.2243e-01,  5.1416e-02, -3.0359e-01,\n","         -3.9074e-01, -2.4934e-01, -2.1908e-01,  2.3396e-01, -6.1765e-02,\n","         -4.9388e-02,  2.6519e-02, -1.6949e-01,  1.2674e-02,  1.8336e-01],\n","        [ 2.3219e-02,  2.0441e-01,  2.6340e-01, -3.9164e-02,  4.1718e-02,\n","         -1.9175e-01,  1.2125e-01,  1.5687e-01, -2.0685e-01,  3.5703e-01,\n","          2.9999e-02,  2.4176e-01,  7.3206e-02,  2.9100e-01,  3.7750e-01,\n","         -6.1510e-02,  1.4496e-01, -6.0916e-03,  5.4424e-02,  1.5871e-01],\n","        [ 7.4129e-02, -7.1057e-02,  9.4110e-02, -1.8343e-01,  2.0369e-01,\n","          4.1151e-01,  2.3607e-01,  1.5189e-01,  1.4459e-01,  1.0155e-01,\n","         -2.6134e-01,  2.1627e-01, -5.2350e-02,  2.2732e-01,  4.6547e-03,\n","          1.6601e-01, -6.4228e-02, -5.0626e-02,  1.1486e-01, -2.5073e-02],\n","        [ 1.0972e-01,  1.0781e-01,  1.5523e-01,  1.9413e-01, -2.4741e-01,\n","          8.4863e-02, -2.8281e-01,  4.8762e-02, -1.5077e-01,  6.5831e-02,\n","          1.7440e-01,  2.9275e-02,  7.4442e-03,  1.8540e-01,  1.9783e-01,\n","          1.5303e-02, -4.0344e-02, -9.8515e-02,  6.8861e-02, -2.8018e-01],\n","        [-7.5318e-02, -2.2478e-01, -1.8136e-01, -3.3546e-01,  3.6278e-02,\n","          3.8203e-01, -3.9075e-01, -2.0561e-01, -7.9670e-02, -1.7235e-01,\n","          5.6182e-02, -1.4073e-01, -7.4428e-04,  1.3301e-01, -2.3893e-01,\n","          1.1729e-01,  1.9434e-01, -8.5142e-02, -6.1296e-03,  8.7096e-02],\n","        [-5.5455e-02, -5.0259e-02,  3.0495e-02,  1.0060e-01,  6.6735e-02,\n","          1.6796e-01,  3.2423e-01,  9.7910e-02,  1.6383e-01,  8.3321e-02,\n","          1.7828e-01, -3.1479e-01, -2.9713e-01, -2.9718e-01, -1.8868e-01,\n","         -7.9471e-02, -1.5405e-01,  2.4528e-01,  4.5686e-02, -8.7389e-03],\n","        [ 2.5327e-01, -2.4031e-02,  3.1926e-01,  3.4604e-01,  1.1738e-01,\n","          1.3930e-01,  4.3254e-03,  1.9030e-01, -1.1297e-01, -2.2284e-01,\n","         -1.7050e-01,  1.0301e-01,  4.0368e-02, -8.8819e-02, -7.4145e-02,\n","          1.2622e-01,  2.4380e-01,  2.2204e-01,  1.5679e-01,  2.6118e-01],\n","        [-1.8566e-01,  2.0052e-01,  2.5719e-01, -2.0088e-01,  1.4777e-01,\n","         -1.4480e-01, -1.3841e-01,  1.3864e-01,  9.6777e-02,  2.7627e-01,\n","          1.2166e-01,  2.8219e-01, -1.6177e-01,  1.9548e-01,  3.1808e-02,\n","          6.3022e-02,  1.9125e-01, -1.1173e-01,  2.2598e-01, -2.4595e-01],\n","        [-1.8437e-01,  3.9266e-02,  2.3985e-01, -5.6223e-02,  3.6612e-01,\n","          1.2527e-02,  2.4885e-01,  8.4621e-02, -9.6763e-02,  2.0748e-01,\n","          4.1542e-01,  2.4091e-01,  3.5730e-01,  7.6473e-02,  2.0431e-01,\n","          1.2087e-02,  1.7020e-01,  3.2124e-01, -1.5872e-02,  4.7498e-02],\n","        [-1.1054e-01,  2.4038e-01,  1.3133e-01, -2.8782e-01,  7.3203e-02,\n","         -3.5685e-01,  1.5520e-01,  2.8861e-01, -1.9122e-01, -9.4792e-02,\n","          2.2668e-01,  3.7634e-02,  4.2471e-02, -1.4443e-01,  1.9300e-01,\n","          8.0012e-02, -1.0844e-01,  3.8268e-01,  1.7628e-03,  3.4079e-01],\n","        [ 3.4603e-02, -3.0290e-01,  1.9193e-01, -3.3389e-01,  1.6330e-01,\n","          3.1751e-02,  1.7152e-01,  5.3826e-02,  3.1274e-01, -3.6381e-01,\n","         -1.2439e-01,  1.4324e-01,  4.2970e-01, -1.1828e-02,  3.2658e-01,\n","         -2.7257e-02,  1.0247e-01, -4.3677e-02,  1.1521e-01, -2.1507e-01],\n","        [-1.2390e-01, -1.4703e-01, -2.4687e-02, -1.9759e-01,  2.0798e-01,\n","         -1.0044e-01,  1.2456e-01,  4.9230e-02, -2.0217e-01,  2.8834e-01,\n","          2.4329e-01,  6.2749e-02, -1.4683e-01, -6.6778e-02, -1.2924e-01,\n","         -1.2826e-01,  1.7793e-01,  3.4948e-01,  2.7391e-01, -4.8894e-02],\n","        [-3.9553e-01,  2.7293e-01, -7.0247e-02, -2.9594e-01,  6.1227e-02,\n","         -3.2009e-01, -1.5574e-01,  3.2276e-02,  1.8874e-01, -2.7772e-01,\n","         -1.6888e-01,  2.8588e-02, -2.6071e-01, -1.6851e-01,  7.3318e-02,\n","          2.6384e-02,  7.5476e-02,  3.6080e-02, -2.9222e-02, -4.1342e-01],\n","        [ 2.0433e-02,  2.0919e-02, -3.0985e-01,  2.0779e-05,  1.1830e-01,\n","         -7.5158e-02,  2.2542e-01,  2.9485e-01,  1.7803e-02,  1.4941e-01,\n","         -1.3357e-01,  1.3093e-01, -4.6043e-01, -1.1853e-02, -8.1371e-02,\n","          1.6211e-01, -1.5489e-02, -8.1079e-02, -1.1301e-01,  3.7041e-02],\n","        [ 2.2869e-01,  2.3413e-01,  2.7738e-01,  2.7480e-01, -1.5667e-01,\n","          3.6032e-01, -1.0995e-01,  1.0015e-01,  1.1975e-01,  8.1215e-02,\n","          4.9474e-02, -1.9420e-01,  6.7955e-03,  1.8705e-01,  3.9182e-02,\n","          1.0601e-02,  1.0453e-01, -1.7935e-01, -1.1296e-01, -2.0000e-01],\n","        [-5.9474e-02, -5.7690e-02,  9.1970e-02, -1.7758e-01, -2.4441e-01,\n","         -1.1228e-01,  1.9918e-01,  1.4525e-01, -1.9332e-02, -5.9073e-03,\n","         -2.6169e-01, -3.5333e-02, -7.7079e-02,  2.0321e-01,  2.4427e-01,\n","         -3.3261e-01,  1.5286e-01, -2.2919e-01, -5.6263e-02, -1.1252e-01],\n","        [-6.1711e-02,  2.4316e-01,  1.4999e-01,  1.6512e-01,  1.7466e-02,\n","          3.0482e-01,  9.7014e-02,  3.0797e-01,  4.4501e-02, -2.1756e-01,\n","          2.7397e-01,  9.3015e-02,  2.6185e-01, -1.7498e-01, -4.2588e-03,\n","          3.3862e-02,  4.7024e-02,  3.7603e-01, -1.3511e-01,  1.6609e-01],\n","        [ 8.0339e-02, -1.1385e-01,  3.0462e-01, -9.3406e-02,  9.9874e-02,\n","         -7.8699e-02, -2.4062e-01, -2.7820e-01,  3.1261e-02,  1.6554e-01,\n","          1.0141e-01,  1.3911e-01, -2.7368e-01,  1.9779e-01, -2.0275e-02,\n","         -7.1733e-03, -2.7925e-01, -3.4498e-01, -1.7945e-02, -4.4300e-02],\n","        [-2.5984e-01, -7.9138e-02,  1.2655e-01,  2.3729e-01, -1.7985e-01,\n","         -1.4420e-01, -5.9679e-02, -2.1148e-01,  6.3261e-02, -1.6593e-01,\n","          1.4545e-01,  2.4052e-01, -3.8935e-03, -1.9706e-01, -4.0059e-01,\n","         -1.8460e-02, -6.8744e-02, -7.5684e-02,  8.3822e-02, -1.5071e-03],\n","        [ 2.2155e-01,  2.8276e-01, -3.5327e-02, -6.8489e-02, -2.0184e-01,\n","         -2.0496e-01, -1.1583e-01, -1.4212e-01,  1.3203e-01,  1.9574e-01,\n","         -2.4465e-01, -3.5349e-02, -2.2339e-01,  2.6979e-01,  1.4432e-01,\n","         -8.5154e-02,  1.1637e-01,  5.4419e-02,  2.7356e-01, -9.0068e-02],\n","        [ 4.1401e-02, -6.1149e-02, -3.0197e-01, -6.5715e-03, -1.3968e-01,\n","          1.2699e-01,  3.6713e-03,  3.6843e-01, -1.4839e-01,  7.7749e-04,\n","          2.2553e-01, -1.7263e-01, -9.7846e-02, -8.0106e-02, -7.0196e-02,\n","         -1.1267e-01, -2.9435e-01,  4.3930e-02,  2.7285e-01,  2.3924e-02],\n","        [ 2.2649e-01,  2.1123e-01,  3.9697e-02,  2.7561e-01, -2.1694e-01,\n","          1.8438e-01,  1.3325e-01,  1.2844e-01,  1.3514e-01, -1.0959e-01,\n","          1.0404e-01,  1.2867e-01, -1.2190e-01,  2.8751e-03, -3.1820e-01,\n","         -3.4041e-01,  1.9023e-02,  2.8862e-01, -1.3292e-01, -3.6493e-02],\n","        [-2.6731e-01,  1.7952e-01, -7.8461e-02,  2.6579e-02, -8.2886e-04,\n","         -8.5092e-02,  2.2473e-01, -3.3597e-02, -3.6723e-01, -1.1449e-01,\n","          1.6179e-03, -3.3475e-02, -3.4075e-02, -2.2064e-01, -2.8845e-01,\n","          6.1278e-02, -1.5329e-01,  1.5236e-01,  2.4581e-01, -1.5169e-02],\n","        [ 1.5282e-01,  1.0093e-03,  6.2041e-02,  3.6558e-01, -2.6790e-01,\n","          2.0658e-01,  1.3588e-01, -1.3556e-01,  4.7815e-02,  7.5996e-02,\n","         -6.5438e-02, -2.3147e-01, -2.5327e-01,  1.0740e-02, -7.1620e-02,\n","         -1.9893e-01, -4.4373e-02, -1.5188e-01,  1.0696e-01, -1.6178e-01]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"oGlgdJLW_lWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728941,"user_tz":-330,"elapsed":1929,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"c47b41fb-1ec9-469a-8454-9beeeaa73dd4"},"source":["(net.layers[0].weight - learner.layers[0].weight)/train_grad[0]"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000],\n","        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n","         0.1000, 0.1000]], grad_fn=<DivBackward0>)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"O00DsN2g_lWx"},"source":["So one step in the diretion of the gradient (w.r.t train_loss) has been taken. Next we compute the loss of this ADAPTED learner w.r.t. the TEST data of the task, i.e., d_test:"]},{"cell_type":"code","metadata":{"id":"E5V54kQB_lWy","executionInfo":{"status":"ok","timestamp":1617944728941,"user_tz":-330,"elapsed":1919,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["test_preds = learner(d_test[0])\n","adapt_loss = lossfn(test_preds,d_test[1])"],"execution_count":47,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDLhyKRU_lWy"},"source":["The main MAML update to the original network net takes place now, by back-propagating through the (cumulative) adaptation loss (across possibly many tasks, here there was just one):"]},{"cell_type":"code","metadata":{"id":"m2cME8fl_lWy","executionInfo":{"status":"ok","timestamp":1617944728942,"user_tz":-330,"elapsed":1911,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["task_count = 1\n","optimizer.zero_grad()\n","total_loss = adapt_loss/task_count\n","total_loss.backward()"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"YR7DJRdG_lWy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728944,"user_tz":-330,"elapsed":1908,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"66708cb5-ed5e-4d92-a5c9-ceb8dea3e279"},"source":["net.layers[0].weight"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.0170,  0.2050, -0.1767, -0.0742,  0.0689, -0.4018, -0.1227, -0.0731,\n","          0.2030, -0.0670, -0.0181,  0.0241, -0.3519, -0.3282,  0.1709,  0.0905,\n","          0.2800,  0.1334,  0.2410, -0.3506],\n","        [-0.0458, -0.1684,  0.1120,  0.2084, -0.2014,  0.1386,  0.1588,  0.0160,\n","          0.3254, -0.1710,  0.0373,  0.2474,  0.1794,  0.1083, -0.0437,  0.2532,\n","          0.3755, -0.0917,  0.0109, -0.0544],\n","        [ 0.1411, -0.1818, -0.1080, -0.0474,  0.2119,  0.0112, -0.0047, -0.2275,\n","         -0.1094,  0.1886,  0.0798, -0.0432,  0.1764, -0.1920,  0.3518, -0.1561,\n","         -0.3709,  0.0619,  0.0253,  0.0599],\n","        [ 0.1448, -0.2944,  0.2867, -0.1107, -0.1414, -0.0796, -0.1252, -0.1100,\n","         -0.1379, -0.1652,  0.1786,  0.0779,  0.1500, -0.2978, -0.0799,  0.1574,\n","          0.2165, -0.0510, -0.1667,  0.1264],\n","        [ 0.3457, -0.3076,  0.0544,  0.0358,  0.0608,  0.1720, -0.0316, -0.1716,\n","         -0.0502, -0.1075, -0.0986,  0.1823, -0.1532,  0.1865, -0.0009,  0.1407,\n","         -0.2278,  0.0048, -0.0515,  0.2553],\n","        [-0.3077, -0.3734,  0.0265,  0.0166, -0.2168, -0.1102,  0.1960, -0.0742,\n","          0.0966, -0.2835,  0.1617,  0.2094,  0.2351, -0.0189, -0.1587, -0.0033,\n","          0.1815, -0.1292,  0.2455,  0.2612],\n","        [-0.1052, -0.2977,  0.0099, -0.2287,  0.2633, -0.0478,  0.0172, -0.2991,\n","         -0.1248, -0.2609, -0.2294, -0.0307, -0.2048,  0.0688, -0.2215,  0.3749,\n","          0.2954, -0.0755, -0.0575, -0.0230],\n","        [-0.0442,  0.1185,  0.1107,  0.1273,  0.2049, -0.0755,  0.1663, -0.2325,\n","         -0.3030, -0.1796, -0.2871, -0.0570,  0.0423, -0.2100, -0.2447, -0.0910,\n","         -0.0206, -0.1715, -0.1992, -0.0267],\n","        [ 0.0902,  0.0472, -0.3160, -0.1415,  0.1742,  0.2432, -0.2250, -0.1638,\n","          0.0113, -0.2978, -0.3506, -0.2910, -0.2033,  0.2061, -0.0487, -0.0352,\n","          0.0033, -0.1716,  0.0483,  0.2004],\n","        [ 0.0404,  0.2355,  0.2849,  0.0229,  0.0323, -0.1791,  0.0856,  0.1606,\n","         -0.2125,  0.3547,  0.0445,  0.2296,  0.0537,  0.2960,  0.3918, -0.0787,\n","          0.1475, -0.0518,  0.0416,  0.1518],\n","        [ 0.0782, -0.0688,  0.0920, -0.1800,  0.2022,  0.4141,  0.2307,  0.1526,\n","          0.1455,  0.1004, -0.2602,  0.2166, -0.0519,  0.2260,  0.0050,  0.1662,\n","         -0.0647, -0.0545,  0.1151, -0.0273],\n","        [ 0.1576,  0.1372,  0.1569,  0.2114, -0.2161,  0.1308, -0.3092,  0.0373,\n","         -0.1945,  0.0722,  0.1902, -0.0035,  0.0199,  0.1595,  0.2095,  0.0082,\n","         -0.0575, -0.0993,  0.0928, -0.2637],\n","        [ 0.0129, -0.2318, -0.2168, -0.3555,  0.1270,  0.3932, -0.3943, -0.3016,\n","         -0.1280, -0.1802,  0.1098, -0.1888,  0.0201,  0.1098, -0.2436,  0.1297,\n","          0.1769, -0.0612,  0.0132,  0.1107],\n","        [-0.0753, -0.0359,  0.0527,  0.1016,  0.0436,  0.1592,  0.3309,  0.1157,\n","          0.1743,  0.0972,  0.1569, -0.3163, -0.3239, -0.2741, -0.1874, -0.0862,\n","         -0.1588,  0.2344,  0.0491,  0.0085],\n","        [ 0.2900, -0.0171,  0.2807,  0.3238,  0.1202,  0.1653, -0.0174,  0.1782,\n","         -0.1179, -0.2273, -0.1659,  0.0995,  0.0736, -0.1137, -0.0800,  0.1449,\n","          0.2323,  0.2318,  0.1771,  0.2517],\n","        [-0.1874,  0.1917,  0.2312, -0.2189,  0.1224, -0.1517, -0.1451,  0.1384,\n","          0.1132,  0.2667,  0.1131,  0.2987, -0.1382,  0.1905,  0.0193,  0.0669,\n","          0.1999, -0.1100,  0.2144, -0.2683],\n","        [-0.1835,  0.0546,  0.2354, -0.0260,  0.3272,  0.0147,  0.2160,  0.1061,\n","         -0.0710,  0.2005,  0.4113,  0.2566,  0.3505,  0.0819,  0.2056,  0.0105,\n","          0.1755,  0.2836, -0.0270,  0.0238],\n","        [-0.1077,  0.2402,  0.1321, -0.2740,  0.0684, -0.3714,  0.1483,  0.2870,\n","         -0.1754, -0.1053,  0.2241,  0.0457,  0.0190, -0.1261,  0.1902,  0.0708,\n","         -0.1002,  0.3626, -0.0218,  0.3320],\n","        [ 0.0559, -0.3088,  0.1835, -0.3155,  0.1872,  0.0301,  0.1621,  0.0334,\n","          0.3100, -0.3799, -0.1036,  0.1419,  0.4258, -0.0140,  0.3285, -0.0288,\n","          0.1095, -0.0525,  0.0975, -0.2287],\n","        [-0.1448, -0.1499, -0.0123, -0.2117,  0.1930, -0.1203,  0.1406,  0.0582,\n","         -0.1897,  0.2967,  0.2234,  0.0674, -0.1676, -0.0448, -0.1352, -0.1358,\n","          0.1781,  0.3492,  0.2709, -0.0343],\n","        [-0.3924,  0.2585, -0.0706, -0.2910,  0.0758, -0.3450, -0.1469,  0.0035,\n","          0.1934, -0.2827, -0.1505,  0.0253, -0.2689, -0.1584,  0.0702,  0.0277,\n","          0.0812,  0.0316, -0.0401, -0.4126],\n","        [ 0.0163,  0.0352, -0.2961,  0.0114,  0.1122, -0.0632,  0.2209,  0.3072,\n","          0.0123,  0.1604, -0.1344,  0.1235, -0.4639, -0.0142, -0.0731,  0.1645,\n","         -0.0236, -0.0862, -0.0969,  0.0466],\n","        [ 0.2669,  0.2683,  0.3021,  0.3215, -0.1360,  0.3846, -0.1364,  0.0733,\n","          0.0774,  0.0963,  0.0814, -0.2383,  0.0080,  0.1744,  0.0551, -0.0040,\n","          0.0910, -0.2031, -0.0930, -0.1752],\n","        [-0.0545, -0.0370,  0.1228, -0.1380, -0.2417, -0.1100,  0.1892,  0.1400,\n","         -0.0354,  0.0033, -0.2500, -0.0560, -0.0958,  0.2113,  0.2557, -0.3500,\n","          0.1511, -0.2534, -0.0578, -0.0977],\n","        [-0.0571,  0.2600,  0.1747,  0.1952,  0.0211,  0.3074,  0.0904,  0.3016,\n","          0.0291, -0.2095,  0.2838,  0.0745,  0.2490, -0.1696,  0.0049,  0.0201,\n","          0.0452,  0.3590, -0.1355,  0.1791],\n","        [ 0.0958, -0.1032,  0.2810, -0.1131,  0.0846, -0.0584, -0.2567, -0.2705,\n","          0.0350,  0.1679,  0.0917,  0.1414, -0.2533,  0.1840, -0.0248,  0.0064,\n","         -0.2898, -0.3404,  0.0016, -0.0488],\n","        [-0.2654, -0.0694,  0.1377,  0.2502, -0.1990, -0.1429, -0.0681, -0.2051,\n","          0.0589, -0.1574,  0.1444,  0.2358,  0.0019, -0.1988, -0.3993, -0.0284,\n","         -0.0686, -0.0873,  0.0875,  0.0025],\n","        [ 0.2160,  0.2656, -0.0563, -0.0892, -0.2037, -0.2095, -0.1104, -0.1373,\n","          0.1480,  0.1868, -0.2485, -0.0173, -0.2133,  0.2651,  0.1378, -0.0710,\n","          0.1196,  0.0668,  0.2720, -0.1050],\n","        [ 0.0168, -0.0516, -0.2534, -0.0026, -0.1156,  0.1143,  0.0430,  0.3583,\n","         -0.1680,  0.0239,  0.2241, -0.2012, -0.1346, -0.0548, -0.0571, -0.1166,\n","         -0.3035,  0.0562,  0.2836,  0.0671],\n","        [ 0.2273,  0.2177,  0.0507,  0.2881, -0.2174,  0.1839,  0.1305,  0.1261,\n","          0.1289, -0.1059,  0.1073,  0.1214, -0.1270,  0.0057, -0.3152, -0.3482,\n","          0.0191,  0.2807, -0.1342, -0.0308],\n","        [-0.3013,  0.1698, -0.0607,  0.0218, -0.0151, -0.1095,  0.2463, -0.0186,\n","         -0.3514, -0.1125, -0.0166, -0.0210, -0.0566, -0.1975, -0.2915,  0.0518,\n","         -0.1442,  0.1524,  0.2290, -0.0116],\n","        [ 0.1645,  0.0240,  0.0854,  0.3921, -0.2626,  0.2181,  0.1251, -0.1379,\n","          0.0278,  0.0854, -0.0611, -0.2527, -0.2670,  0.0143, -0.0624, -0.2151,\n","         -0.0501, -0.1677,  0.1106, -0.1448]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"y6CbgDTR_lWy","executionInfo":{"status":"ok","timestamp":1617944728945,"user_tz":-330,"elapsed":1900,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["optimizer.step()"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6UdChbh_lWy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617944728946,"user_tz":-330,"elapsed":1897,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"5b84d4e8-18bb-4172-e49d-04b8953bec7b"},"source":["net.layers[0].weight"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-1.6025e-02,  2.0403e-01, -1.7575e-01, -7.3193e-02,  6.9858e-02,\n","         -4.0276e-01, -1.2170e-01, -7.4097e-02,  2.0399e-01, -6.7991e-02,\n","         -1.7143e-02,  2.3123e-02, -3.5086e-01, -3.2717e-01,  1.6986e-01,\n","          9.1503e-02,  2.8096e-01,  1.3241e-01,  2.3999e-01, -3.4956e-01],\n","        [-4.6808e-02, -1.6940e-01,  1.1296e-01,  2.0938e-01, -2.0242e-01,\n","          1.3758e-01,  1.5976e-01,  1.4998e-02,  3.2643e-01, -1.6996e-01,\n","          3.8332e-02,  2.4843e-01,  1.8040e-01,  1.0725e-01, -4.2651e-02,\n","          2.5418e-01,  3.7645e-01, -9.2662e-02,  1.1851e-02, -5.5411e-02],\n","        [ 1.4210e-01, -1.8082e-01, -1.0904e-01, -4.6396e-02,  2.1092e-01,\n","          1.2237e-02, -5.7000e-03, -2.2649e-01, -1.1043e-01,  1.8757e-01,\n","          8.0756e-02, -4.4161e-02,  1.7744e-01, -1.9300e-01,  3.5284e-01,\n","         -1.5712e-01, -3.7193e-01,  6.0922e-02,  2.6269e-02,  5.8857e-02],\n","        [ 1.4378e-01, -2.9545e-01,  2.8771e-01, -1.0974e-01, -1.4036e-01,\n","         -8.0591e-02, -1.2417e-01, -1.1103e-01, -1.3691e-01, -1.6624e-01,\n","          1.7956e-01,  7.8930e-02,  1.5103e-01, -2.9876e-01, -8.0915e-02,\n","          1.5842e-01,  2.1749e-01, -5.0010e-02, -1.6567e-01,  1.2744e-01],\n","        [ 3.4670e-01, -3.0661e-01,  5.3398e-02,  3.6757e-02,  5.9826e-02,\n","          1.7298e-01, -3.2619e-02, -1.7057e-01, -4.9210e-02, -1.0850e-01,\n","         -9.9591e-02,  1.8330e-01, -1.5218e-01,  1.8554e-01,  1.0206e-04,\n","          1.3973e-01, -2.2684e-01,  3.7757e-03, -5.2529e-02,  2.5427e-01],\n","        [-3.0667e-01, -3.7440e-01,  2.5500e-02,  1.7556e-02, -2.1581e-01,\n","         -1.1119e-01,  1.9499e-01, -7.5197e-02,  9.7636e-02, -2.8253e-01,\n","          1.6267e-01,  2.0839e-01,  2.3615e-01, -1.7858e-02, -1.5770e-01,\n","         -4.3206e-03,  1.8051e-01, -1.3018e-01,  2.4450e-01,  2.6016e-01],\n","        [-1.0423e-01, -2.9667e-01,  1.0869e-02, -2.2973e-01,  2.6231e-01,\n","         -4.8773e-02,  1.6250e-02, -3.0014e-01, -1.2376e-01, -2.6187e-01,\n","         -2.3038e-01, -3.1691e-02, -2.0377e-01,  6.9801e-02, -2.2047e-01,\n","          3.7594e-01,  2.9638e-01, -7.6507e-02, -5.8454e-02, -2.3998e-02],\n","        [-4.5195e-02,  1.1746e-01,  1.0971e-01,  1.2630e-01,  2.0591e-01,\n","         -7.6465e-02,  1.6729e-01, -2.3352e-01, -3.0204e-01, -1.7855e-01,\n","         -2.8614e-01, -5.5980e-02,  4.3277e-02, -2.1103e-01, -2.4365e-01,\n","         -9.0008e-02, -1.9598e-02, -1.7050e-01, -1.9817e-01, -2.5677e-02],\n","        [ 8.9211e-02,  4.8176e-02, -3.1699e-01, -1.4245e-01,  1.7318e-01,\n","          2.4421e-01, -2.2401e-01, -1.6280e-01,  1.2310e-02, -2.9684e-01,\n","         -3.5159e-01, -2.9001e-01, -2.0228e-01,  2.0513e-01, -4.9717e-02,\n","         -3.4192e-02,  4.2885e-03, -1.7062e-01,  4.9334e-02,  1.9939e-01],\n","        [ 3.9444e-02,  2.3454e-01,  2.8594e-01,  2.1864e-02,  3.3309e-02,\n","         -1.8006e-01,  8.6637e-02,  1.5962e-01, -2.1352e-01,  3.5574e-01,\n","          4.5493e-02,  2.2865e-01,  5.2720e-02,  2.9500e-01,  3.9079e-01,\n","         -7.7667e-02,  1.4645e-01, -5.0756e-02,  4.2594e-02,  1.5284e-01],\n","        [ 7.9187e-02, -6.9757e-02,  9.1006e-02, -1.7902e-01,  2.0325e-01,\n","          4.1510e-01,  2.2971e-01,  1.5355e-01,  1.4648e-01,  9.9411e-02,\n","         -2.5915e-01,  2.1762e-01, -5.2926e-02,  2.2703e-01,  5.9742e-03,\n","          1.6721e-01, -6.3660e-02, -5.5489e-02,  1.1405e-01, -2.8325e-02],\n","        [ 1.5857e-01,  1.3821e-01,  1.5593e-01,  2.1038e-01, -2.1709e-01,\n","          1.2976e-01, -3.0820e-01,  3.8322e-02, -1.9349e-01,  7.1226e-02,\n","          1.8916e-01, -2.4529e-03,  1.8906e-02,  1.5851e-01,  2.0848e-01,\n","          9.1993e-03, -5.6550e-02, -9.8281e-02,  9.1777e-02, -2.6475e-01],\n","        [ 1.3898e-02, -2.3082e-01, -2.1782e-01, -3.5454e-01,  1.2600e-01,\n","          3.9419e-01, -3.9526e-01, -3.0064e-01, -1.2704e-01, -1.7924e-01,\n","          1.0881e-01, -1.8784e-01,  2.1056e-02,  1.0881e-01, -2.4460e-01,\n","          1.3069e-01,  1.7590e-01, -6.2162e-02,  1.4172e-02,  1.1171e-01],\n","        [-7.4324e-02, -3.6929e-02,  5.1665e-02,  1.0256e-01,  4.2636e-02,\n","          1.5822e-01,  3.2988e-01,  1.1667e-01,  1.7530e-01,  9.6193e-02,\n","          1.5786e-01, -3.1531e-01, -3.2290e-01, -2.7508e-01, -1.8838e-01,\n","         -8.7213e-02, -1.5776e-01,  2.3338e-01,  4.8075e-02,  7.5169e-03],\n","        [ 2.9100e-01, -1.8146e-02,  2.7968e-01,  3.2276e-01,  1.2116e-01,\n","          1.6427e-01, -1.8421e-02,  1.7719e-01, -1.1890e-01, -2.2631e-01,\n","         -1.6691e-01,  1.0054e-01,  7.2618e-02, -1.1273e-01, -7.8988e-02,\n","          1.4590e-01,  2.3335e-01,  2.3278e-01,  1.7609e-01,  2.5269e-01],\n","        [-1.8842e-01,  1.9068e-01,  2.3218e-01, -2.1990e-01,  1.2338e-01,\n","         -1.5275e-01, -1.4410e-01,  1.3936e-01,  1.1224e-01,  2.6773e-01,\n","          1.1413e-01,  2.9969e-01, -1.3724e-01,  1.8950e-01,  2.0285e-02,\n","          6.5876e-02,  1.9890e-01, -1.0904e-01,  2.1543e-01, -2.6729e-01],\n","        [-1.8253e-01,  5.3602e-02,  2.3639e-01, -2.7035e-02,  3.2817e-01,\n","          1.3693e-02,  2.1700e-01,  1.0506e-01, -7.1976e-02,  2.0146e-01,\n","          4.1228e-01,  2.5565e-01,  3.4950e-01,  8.2854e-02,  2.0665e-01,\n","          1.1537e-02,  1.7646e-01,  2.8457e-01, -2.7982e-02,  2.4815e-02],\n","        [-1.0666e-01,  2.3921e-01,  1.3315e-01, -2.7503e-01,  6.9444e-02,\n","         -3.7242e-01,  1.4931e-01,  2.8604e-01, -1.7644e-01, -1.0431e-01,\n","          2.2313e-01,  4.4671e-02,  1.7984e-02, -1.2511e-01,  1.9123e-01,\n","          7.1817e-02, -9.9196e-02,  3.6356e-01, -2.2812e-02,  3.3302e-01],\n","        [ 5.4874e-02, -3.0775e-01,  1.8447e-01, -3.1645e-01,  1.8819e-01,\n","          2.9066e-02,  1.6313e-01,  3.2374e-02,  3.0896e-01, -3.7894e-01,\n","         -1.0464e-01,  1.4093e-01,  4.2477e-01, -1.3020e-02,  3.2946e-01,\n","         -2.7774e-02,  1.1054e-01, -5.1524e-02,  9.6457e-02, -2.2775e-01],\n","        [-1.4376e-01, -1.4887e-01, -1.3268e-02, -2.1070e-01,  1.9205e-01,\n","         -1.1927e-01,  1.3959e-01,  5.9242e-02, -1.8870e-01,  2.9571e-01,\n","          2.2439e-01,  6.8403e-02, -1.6656e-01, -4.5811e-02, -1.3620e-01,\n","         -1.3675e-01,  1.7912e-01,  3.4822e-01,  2.6993e-01, -3.5327e-02],\n","        [-3.9140e-01,  2.5750e-01, -6.9569e-02, -2.9202e-01,  7.6787e-02,\n","         -3.4598e-01, -1.4587e-01,  2.4789e-03,  1.9442e-01, -2.8371e-01,\n","         -1.4948e-01,  2.4262e-02, -2.6791e-01, -1.5745e-01,  7.1169e-02,\n","          2.6704e-02,  8.2241e-02,  3.0615e-02, -4.1086e-02, -4.1159e-01],\n","        [ 1.7275e-02,  3.4216e-02, -2.9711e-01,  1.0432e-02,  1.1123e-01,\n","         -6.4200e-02,  2.1995e-01,  3.0617e-01,  1.3303e-02,  1.5942e-01,\n","         -1.3335e-01,  1.2449e-01, -4.6285e-01, -1.5248e-02, -7.2053e-02,\n","          1.6350e-01, -2.2567e-02, -8.5150e-02, -9.7898e-02,  4.5569e-02],\n","        [ 2.6785e-01,  2.6726e-01,  3.0109e-01,  3.2045e-01, -1.3698e-01,\n","          3.8364e-01, -1.3543e-01,  7.4317e-02,  7.8365e-02,  9.5329e-02,\n","          8.0445e-02, -2.3733e-01,  9.0062e-03,  1.7340e-01,  5.4059e-02,\n","         -3.0302e-03,  9.2029e-02, -2.0208e-01, -9.4016e-02, -1.7616e-01],\n","        [-5.5457e-02, -3.5954e-02,  1.2378e-01, -1.3901e-01, -2.4065e-01,\n","         -1.1096e-01,  1.9023e-01,  1.3900e-01, -3.6432e-02,  4.2608e-03,\n","         -2.5104e-01, -5.7012e-02, -9.6844e-02,  2.1227e-01,  2.5471e-01,\n","         -3.4900e-01,  1.5008e-01, -2.5239e-01, -5.6773e-02, -9.6655e-02],\n","        [-5.6150e-02,  2.6102e-01,  1.7369e-01,  1.9420e-01,  2.0131e-02,\n","          3.0641e-01,  9.1377e-02,  3.0056e-01,  3.0072e-02, -2.1052e-01,\n","          2.8278e-01,  7.5498e-02,  2.5004e-01, -1.7063e-01,  3.9100e-03,\n","          2.1078e-02,  4.6239e-02,  3.5796e-01, -1.3649e-01,  1.7810e-01],\n","        [ 9.4817e-02, -1.0416e-01,  2.8199e-01, -1.1409e-01,  8.5605e-02,\n","         -5.9445e-02, -2.5573e-01, -2.7152e-01,  3.6013e-02,  1.6692e-01,\n","          9.2653e-02,  1.4244e-01, -2.5225e-01,  1.8299e-01, -2.3757e-02,\n","          5.3872e-03, -2.8884e-01, -3.3940e-01,  6.0584e-04, -4.7847e-02],\n","        [-2.6641e-01, -7.0440e-02,  1.3672e-01,  2.5117e-01, -1.9997e-01,\n","         -1.4395e-01, -6.7149e-02, -2.0607e-01,  5.9864e-02, -1.5844e-01,\n","          1.4542e-01,  2.3679e-01,  2.8761e-03, -1.9785e-01, -4.0025e-01,\n","         -2.7440e-02, -6.7606e-02, -8.8328e-02,  8.8548e-02,  1.5374e-03],\n","        [ 2.1498e-01,  2.6464e-01, -5.7318e-02, -9.0156e-02, -2.0467e-01,\n","         -2.1049e-01, -1.0941e-01, -1.3827e-01,  1.4903e-01,  1.8779e-01,\n","         -2.4950e-01, -1.6341e-02, -2.1229e-01,  2.6608e-01,  1.3680e-01,\n","         -6.9958e-02,  1.1861e-01,  6.7817e-02,  2.7304e-01, -1.0597e-01],\n","        [ 1.7767e-02, -5.0567e-02, -2.5439e-01, -1.6420e-03, -1.1663e-01,\n","          1.1530e-01,  4.2020e-02,  3.5926e-01, -1.6695e-01,  2.2948e-02,\n","          2.2508e-01, -2.0016e-01, -1.3359e-01, -5.5763e-02, -5.6054e-02,\n","         -1.1759e-01, -3.0449e-01,  5.5206e-02,  2.8459e-01,  6.6066e-02],\n","        [ 2.2626e-01,  2.1666e-01,  4.9668e-02,  2.8712e-01, -2.1839e-01,\n","          1.8290e-01,  1.3151e-01,  1.2714e-01,  1.2987e-01, -1.0692e-01,\n","          1.0633e-01,  1.2240e-01, -1.2595e-01,  4.6584e-03, -3.1622e-01,\n","         -3.4722e-01,  2.0088e-02,  2.8173e-01, -1.3515e-01, -3.1773e-02],\n","        [-3.0035e-01,  1.7083e-01, -6.1680e-02,  2.2805e-02, -1.6078e-02,\n","         -1.0848e-01,  2.4527e-01, -1.7567e-02, -3.5040e-01, -1.1352e-01,\n","         -1.5634e-02, -2.0040e-02, -5.5628e-02, -1.9848e-01, -2.9251e-01,\n","          5.2834e-02, -1.4318e-01,  1.5144e-01,  2.2802e-01, -1.2617e-02],\n","        [ 1.6554e-01,  2.3024e-02,  8.4370e-02,  3.9106e-01, -2.6363e-01,\n","          2.1714e-01,  1.2409e-01, -1.3893e-01,  2.8820e-02,  8.6365e-02,\n","         -6.2118e-02, -2.5169e-01, -2.6601e-01,  1.5287e-02, -6.1440e-02,\n","         -2.1414e-01, -4.9079e-02, -1.6674e-01,  1.0956e-01, -1.4583e-01]],\n","       requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"sPzZrUu2_lWz"},"source":["So, the original parameters have been updated by a gradient step using on all the task adaptation losses. "]},{"cell_type":"markdown","metadata":{"id":"WGjkmMan_lWz"},"source":["# Putting it all together: MAML Algorithm\n","Now let's put all of the above in a loop - the MAML algorithm:"]},{"cell_type":"code","metadata":{"id":"-nTtZdWb_lWz","executionInfo":{"status":"ok","timestamp":1617944741412,"user_tz":-330,"elapsed":952,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["import learn2learn as l2l\n","import torch.optim as optim\n","classes_train = [i for i in range(5)]\n","classes_test = [i+3 for i in range(5)]\n","classes_train, classes_test\n","shots,ways = 5,2\n","net = models.MLP(dims=[20,64,32,ways])\n","#net = models.RNN(n_classes=3,dim=10,n_layers=2)\n","maml = l2l.algorithms.MAML(net, lr=1e-2)\n","optimizer = optim.Adam(maml.parameters(),lr=5e-3)\n","lossfn = torch.nn.NLLLoss()\n","meta_train_kloader=KShotLoader(meta_train_ds,shots=shots,ways=ways,num_tasks=1000)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWjf7yYD_lW0","executionInfo":{"status":"ok","timestamp":1617944745289,"user_tz":-330,"elapsed":1106,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":["# Number of epochs, tasks per step and number of fast_adaptation steps \n","n_epochs=50\n","task_count=32\n","fas = 5"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PCvPut5I_lW0"},"source":["Note: In practice we use more than one gradient step for adpation, this is called 'fast adaptation'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KY_Dnyh1_lW0","executionInfo":{"status":"ok","timestamp":1617944781486,"user_tz":-330,"elapsed":32414,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"0ccdd0b3-2ef4-4d3b-d79c-a83361cc7472"},"source":["epoch=0\n","while epoch<n_epochs:\n","    adapt_loss = 0.0\n","    test_acc = 0.0\n","    # Sample and train on a task\n","    for task in range(task_count):\n","        d_train,d_test=meta_train_kloader.get_task()\n","        learner = maml.clone()\n","        for fas_step in range(fas):\n","            train_preds = learner(d_train[0])\n","            train_loss = lossfn(train_preds,d_train[1])\n","            learner.adapt(train_loss)\n","        test_preds = learner(d_test[0])\n","        adapt_loss += lossfn(test_preds,d_test[1])\n","        learner.eval()\n","        test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n","        learner.train()\n","        # Done with a task\n","    # Update main network\n","    print('Epoch  % 2d Loss: %2.5e Avg Acc: %2.5f'%(epoch,adapt_loss/task_count,test_acc/task_count))\n","    display.clear_output(wait=True)\n","    optimizer.zero_grad()\n","    total_loss = adapt_loss\n","    total_loss.backward()\n","    optimizer.step()\n","    epoch+=1\n","    "],"execution_count":54,"outputs":[{"output_type":"stream","text":["Epoch   49 Loss: 9.39257e-02 Avg Acc: 0.97187\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSSkbzC7VOEm","executionInfo":{"status":"ok","timestamp":1617949992460,"user_tz":-330,"elapsed":1118,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"584181b7-a2c9-4984-c854-5f7af4f1de71"},"source":["# for i in range(1,5):\n","#   print('Epoch  % d'%(i))\n","#   display.clear_output(wait=True)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Epoch   4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qVFJu2Qk_lW1"},"source":["Now test the trained maml network and applying the adaption step to tasks sampled from the meta_test_ds dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fW8E_cbU_lW1","executionInfo":{"status":"ok","timestamp":1617944785520,"user_tz":-330,"elapsed":1011,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}},"outputId":"ea68df08-1942-4248-cfda-cff894cc5124"},"source":["meta_test_kloader=KShotLoader(meta_test_ds,shots=shots,ways=ways)\n","test_acc = 0.0\n","task_count = 20\n","adapt_steps = 5\n","maml.eval()\n","# Sample and train on a task\n","for task in range(task_count):\n","    d_train,d_test=meta_test_kloader.get_task()\n","    learner = maml.clone()\n","    learner.eval()\n","    for adapt_step in range(adapt_steps):\n","        train_preds = learner(d_train[0])\n","        train_loss = lossfn(train_preds,d_train[1])\n","        learner.adapt(train_loss)\n","    test_preds = learner(d_test[0])\n","    test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n","    # Done with a task\n","learner.train()\n","print('Avg Acc: %2.5f'%(test_acc/task_count))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Avg Acc: 0.96500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"spvq_uNsN0yI","executionInfo":{"status":"ok","timestamp":1617937881004,"user_tz":-330,"elapsed":13088,"user":{"displayName":"Shubham Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijAdgXGWwoUffD5VV0pixqu6iYKebE97tJptk0LQ=s64","userId":"14189324627242244147"}}},"source":[""],"execution_count":104,"outputs":[]}]}